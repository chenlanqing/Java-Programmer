# 一、大模型语言

- [开源大模型](https://huggingface.co/)
- [OpenAI文档](https://platform.openai.com/docs/concepts)
- [评估大模型的指标、框架和数据集](https://github.com/openai/evals)

## 1、什么是大语言模型

大语言模型是一种人工智能模型，通常使用深度学习技术，比如神经网络，来理解和生成人类语言。这些模型的“大”在于它们的参数数量非常多，可以达到数十亿甚至更多，这使得它们能够理解和生成高度复杂的语言模式；

可以**将大语言模型想象成一个巨大的预测机器，其训练过程主要基于“猜词”**：给定一段文本的开头，它的任务就是预测下一个词是什么。模型会根据大量的训练数据（例如在互联网上爬取的文本），试图理解词语和词组在语言中的用法和含义，以及它们如何组合形成意义。它会通过不断地学习和调整参数，使得自己的预测越来越准确；

LangChain 是一个全方位的、基于大语言模型这种预测能力的应用开发工具，它的灵活性和模块化特性使得处理语言模型变得极其简便。不论你在何时何地，都能利用它流畅地调用语言模型，并基于语言模型的“预测”或者说“推理”能力开发新的应用；

# 二、LangChain

- [LangChain](https://github.com/langchain-ai/langchain)
- [LangChain-Java版](https://github.com/Starcloud-Cloud/java-langchain)
- [LangChain4J](https://github.com/langchain4j/langchain4j)
- [LangChain-Introduction](https://python.langchain.com/v0.2/docs/introduction/)

## 1、概述

LangChain 本质上就是对各种大模型提供的 API 的套壳，是为了方便我们使用这些 API，搭建起来的一些框架、模块和接口。

因此，要了解 LangChain 的底层逻辑，需要了解大模型的 API 的基本设计思路。而目前接口最完备的、同时也是最强大的大语言模型，当然是 OpenAI 提供的 GPT 家族模型

## 2、安装LangChain

以Python为例：
```
pip install langchain
```
LangChain 要与各种模型、数据存储库集成，当你 `pip install langchain` 之后，可能还需要 `pip install openai`、`pip install chroma`（一种向量数据库）

安装 LangChain 时包括常用的开源 LLM（大语言模型） 库：
```
pip install langchain[llms]
```
安装完成之后，还需要更新到 LangChain 的最新版本，这样才能使用较新的工具
```
pip install --upgrade langchain
```

## 3、OpenAI

OpenAI主要模型Chat Model 和 Text Model，这两类 Model，是大语言模型的代表。当然，OpenAI 还提供 Image、Audio 和其它类型的模型，目前它们不是 LangChain 所支持的重点，模型数量也比较少
- Chat Model，聊天模型：用于产生人类和 AI 之间的对话，代表模型当然是 gpt-3.5-turbo（也就是 ChatGPT）和 GPT-4；
- Text Model，文本模型，在 ChatGPT 出来之前，大家都使用这种模型的 API 来调用 GPT-3，文本模型的代表作是 text-davinci-003（基于 GPT3）

### 3.1、调用Text模型

- 注册好OpenAI的API key；
- 安装OpenAI: `pip install openai`;
- 导入 OpenAI API Key:
    ```python
    import os
    # OpenAI 库就会查看名为 OPENAI_API_KEY 的环境变量，并使用它的值作为 API 密钥
    os.environ["OPENAI_API_KEY"] = '你的Open API Key' #
    ```
- 导入 OpenAI 库，并创建一个 Client
    ```python
    from openai import OpenAI
    client = OpenAI()
    ```
- 指定 gpt-3.5-turbo-instruct（也就是 Text 模型）并调用 completions 方法，返回结果。
    ```python
    response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    temperature=0.5,
    max_tokens=100,
    prompt="请给我的花店起个名")
    # 从响应中获取第一个（如果在调用大模型时，没有指定 n 参数，那么就只有唯一的一个响应）选择，然后获取该选择的文本，并移除其前后的空白字符
    print(response.choices[0].text.strip())
    ```

OpenAI的Text模型 client.completions.create 请求参数：

![](image/OpenAI-Text-模型参数.png)

在使用 Text 模型（如 text-davinci-003）的情况下，响应对象的主要字段包括：

![](image/OpenAI-Text-模型-响应参数.png)

### 3.2、调用Chat模型

整体流程上，Chat 模型和 Text 模型的调用是类似的，只是前面加了一个 chat，然后输入（prompt）和输出（response）的数据格式有所不同：
```python
response = client.chat.completions.create(  
  model="gpt-4",
  messages=[
        {"role": "system", "content": "You are a creative AI."},
        {"role": "user", "content": "请给我的花店起个名"},
    ],
  temperature=0.8,
  max_tokens=60
)
```
有两个专属于 Chat 模型的概念，一个是消息，一个是角色：
- **消息**：就是传入模型的提示。此处的 messages 参数是一个列表，包含了多个消息。每个消息都有一个 role（可以是 system、user 或 assistant）和 content（消息的内容）。系统消息设定了对话的背景（你是一个很棒的智能助手），然后用户消息提出了具体请求（请给我的花店起个名）。模型的任务是基于这些消息来生成回复；
- **角色**：在 OpenAI 的 Chat 模型中，system、user 和 assistant 都是消息的角色。每一种角色都有不同的含义和作用；
    - system：系统消息主要用于设定对话的背景或上下文。这可以帮助模型理解它在对话中的角色和任务。例如，你可以通过系统消息来设定一个场景，让模型知道它是在扮演一个医生、律师或者一个知识丰富的 AI 助手。系统消息通常在对话开始时给出。
    - user：用户消息是从用户或人类角色发出的。它们通常包含了用户想要模型回答或完成的请求。用户消息可以是一个问题、一段话，或者任何其他用户希望模型响应的内容。
    - assistant：助手消息是模型的回复。例如，在你使用 API 发送多轮对话中新的对话请求时，可以通过助手消息提供先前对话的上下文。然而，请注意在对话的最后一条消息应始终为用户消息，因为模型总是要回应最后这条用户消息

在使用 Chat 模型生成内容后，返回的响应，也就是 response 会包含一个或多个 choices：
```json
{
 'id': 'chatcmpl-2nZI6v1cW9E3Jg4w2Xtoql0M3XHfH',
 'object': 'chat.completion',
 'created': 1677649420,
 'model': 'gpt-4',
 'usage': {'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87},
 'choices': [
   {
    'message': {
      'role': 'assistant',
      'content': '你的花店可以叫做"花香四溢"。'
     },
    'finish_reason': 'stop',
    'index': 0
   }
  ]
}
```

### 3.3、Chat 模型 vs Text 模型

相较于 Text 模型，Chat 模型的设计更适合处理对话或者多轮次交互的情况。这是因为它可以接受一个消息列表作为输入，而不仅仅是一个字符串。这个消息列表可以包含 system、user 和 assistant 的历史信息，从而在处理交互式对话时提供更多的上下文信息；

对于简单的单轮文本生成任务，使用 Text 模型可能会更简单、更直接。例如，如果你只需要模型根据一个简单的提示生成一段文本，那么 Text 模型可能更适合

## 4、通过LangChain调用

**调用Text模型：**
```python
import os
os.environ["OPENAI_API_KEY"] = '你的Open API Key'
from langchain.llms import OpenAI
llm = OpenAI(  
    model="gpt-3.5-turbo-instruct",
    temperature=0.8,
    max_tokens=60,)
response = llm.predict("请给我的花店起个名")
print(response)
```

**调用 Chat 模型**
```python
import os
os.environ["OPENAI_API_KEY"] = '你的Open API Key'
from langchain.chat_models import ChatOpenAI
chat = ChatOpenAI(model="gpt-4",
                    temperature=0.8,
                    max_tokens=60)
from langchain.schema import (
    HumanMessage,
    SystemMessage
)
messages = [
    SystemMessage(content="你是一个很棒的智能助手"),
    HumanMessage(content="请给我的花店起个名")
]
response = chat(messages)
print(response)
```

## 5、调用开源社区模型

要选择 Text-Generation、Text-Text Generation 和 Question-Answer 这一类的文本生成式模型：
```python
from langchain import HuggingFaceHub
llm = HuggingFaceHub(model_id="bigscience/bloom-1b7")
```


# 参考资料

- [BCEmbedding: Bilingual and Crosslingual Embedding for RAG](https://github.com/netease-youdao/BCEmbedding)
- [大模型训练](https://github.com/hiyouga/LLaMA-Efficient-Tuning)
- [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter)
- [量化 LLM 的高效微调](https://github.com/artidoro/qlora)
