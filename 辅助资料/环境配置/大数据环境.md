# 1、Hadoop环境搭建

前提条件：
- JDK
- 免密SSH登录
- 设置hostname

## 1.1、Hadoop2.x环境



### 1.1.1、伪集群安装

**环境**
- 系统：CentOS7.4
- Hadoop版本：hadoop-2.7.0
- JDK版本：1.8.0_231
- 机器IP，静态地址：192.168.89.145
- hostname：hadoop
- 免密SSH登录

**配置步骤**
- 解压 Hadoop-2.7.0
- 配置 `etc/hadoop/hadoop-env.sh`：配置 `export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64`，即设置Java的安装目录；
- 配置文件 `etc/hadoop/hdfs-site.xml`：
    ```xml
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        <property>
            <name>dfs.permissions.enabled</name>
            <value>false</value>
        </property>
    </configuration>
    ```
- 配置文件 `etc/hadoop/core-site.xml`：
    ```xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <!-- hdfs的目录地址，这里配置的是hostname:port  -->
            <value>hdfs://bluefish:9000</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <!-- hadoop的临时目录 -->
            <value>/root/data-warehouse/tmp/hadoop</value>
        </property>
    </configuration>
    ```
- 配置`etc/hadoop/mapred-site.xml`：
    ```xml
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    ```
- 配置`etc/hadoop/yarn-site.xml`：
    ```xml
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    ```
- 配置文件 `etc/hadoop/slaves`：单机的话，配置当前机器的 hostname 即可；
- 格式化hdfs：`bin/hdfs namenode -format`
- 启动hdfs
    - 启动namnode： `sbin/hadoop-daemon.sh start namenode`
    - 启动datanode： `sbin/hadoop-daemon.sh start datanode`

    或者直接启动：`sbin/start-dfs.sh`
- 启动yarn：`sbin/start-yarn.sh`

    yarn访问地址：http://bigdata1:8088

### 1.1.2、集群安装

**环境**
- 系统：CentOS7.4
- Hadoop版本：hadoop-2.6.0-cdh5.14.4
- JDK版本：1.8.0_231
- 三台机器：
    - 192.168.89.141 hadoop001（主）
    - 192.168.89.142 hadoop002
    - 192.168.89.143 hadoop003
- 免密SSH登录，需要hadoop001能够免密登录hadoop002、hadoop003

## 1.2、Hadoop3.x环境

2.x对比3.x端口变化

2.x端口	| 3.x端口	| name| 	desc
-------|---------|-------|--------
50470	|9871|	dfs.namenode.https-address|	The namenode secure http server address and port.
50070	|9870	|dfs.namenode.http-address|	The address and the base port where the dfs namenode web ui will listen on.
8020	|9820|	fs.defaultFS	|指定HDFS运行时nameNode地址

### 1.2.1、伪集群安装

**环境**
- 系统：CentOS7.4
- Hadoop版本：3.2.0
- JDK版本：1.8.0_231
- 机器IP，静态地址：192.168.89.141
- hostname：hadoop001
- 免密SSH登录

**配置步骤**
- 解压缩 hadoop.3.2.tar.gz 到目录：`/data/soft/`
- 创建目录：`mkdir -p /data/hadoop_repo/logs/hadoop`
- 修改 `etc/hadoop/hadoop-env.sh`配置，增加环境变量信息：
    ```
    export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64
    export HADOOP_LOG_DIR=/data/hadoop_repo/logs/hadoop
    ```
- 修改 `etc/hadoop/core-site.xml`，注意 `fs.defaultFS` 属性中的主机名需要和你配置的主机名保持一致：
    ```xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop001:9000</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/data/hadoop_repo</value>
        </property>
    </configuration>
    ```
- 修改 `etc/hdfs-site.xml`，把 hdfs 中文件副本的数量设置为 1，因为现在伪分布集群只有一 个节点
    ```xml
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    </configuration>
    ```    

- 修改 `etc/mapred-site.xml`，设置 mapreduce 使用的资源调度框架
    ```xml
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>
    ```

- 修改 `etc/yarn-site.xml`，设置 yarn 上支持运行的服务和环境变量白名单
    ```xml
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.env-whitelist</name>
            <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CL ASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
        </property>
    </configuration>
    ```
- 格式化 namenode： `bin/hdfs namenode -format`

    如果在后面的日志信息中能看到这一行，则说明 namenode 格式化成功。 `common.Storage: Storage directory /data/hadoop_repo/dfs/name has been successfully formatted.`
- 启动hadoop：`sbin/start-all.sh`，直接启动会报错，报错信息：
    ```
    [root@hadoop001 hadoop-3.2.0]# sbin/start-all.sh
    ERROR: Attempting to operate on hdfs namenode as root
    ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
    Starting datanodes
    ERROR: Attempting to operate on hdfs datanode as root
    ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
    Starting secondary namenodes [hadoop100]
    ERROR: Attempting to operate on hdfs secondarynamenode as root
    ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation. 2019-07-25 10:04:25,993 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    Starting resourcemanager
    ERROR: Attempting to operate on yarn resourcemanager as root
    ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
    Starting nodemanagers
    ERROR: Attempting to operate on yarn nodemanager as root
    ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation
    ```
- 修改脚本：`sbin/start-dfs.sh, sbin/stop-dfs.sh`，增加如下内容：
    ```
    HDFS_DATANODE_USER=root
    HDFS_DATANODE_SECURE_USER=hdfs
    HDFS_NAMENODE_USER=root
    HDFS_SECONDARYNAMENODE_USER=root
    ```

- 修改脚本：`sbin/start-yarn.sh, sbin/stop-yarn.sh`，增加如下内容：
    ```
    YARN_RESOURCEMANAGER_USER=root
    HADOOP_SECURE_DN_USER=yarn
    YARN_NODEMANAGER_USER=root
    ```
- 启动集群：`sbin/start-all.sh`
- 验证集群是否正常：`jps`，或者通过页面访问
    ```
    # 执行 jps 命令可以查看集群的进程信息，抛出 Jps 这个进程之外还需要有 5 个进程才说明 集群是正常启动的
    [root@hadoop001 sbin]# jps
    2882 ResourceManager
    2420 DataNode
    3365 Jps
    2619 SecondaryNameNode
    2315 NameNode
    2988 NodeManager
    ```
    还可以通过 webui 界面来验证集群服务是否正常:
    - hdfs webui 界面: http://192.168.89.141:9870
    - yarn webui 界面: http://192.168.89.141:8088

### 1.2.2、集群部署安装

**环境**
- 系统：CentOS7.4
- Hadoop版本：3.2.0
- JDK版本：1.8.0_231
- 三台机器：
    - 192.168.89.141 hadoop001（主）
    - 192.168.89.142 hadoop002
    - 192.168.89.143 hadoop003
- 免密SSH登录，需要hadoop001能够免密登录hadoop002、hadoop003

**配置步骤**
- 解压缩 hadoop.3.2.tar.gz 到目录：`/data/soft/`
- 创建目录：`mkdir -p /data/hadoop_repo/logs/hadoop`
- 修改 `etc/hadoop-env.sh`配置，增加环境变量信息：
    ```
    export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64
    export HADOOP_LOG_DIR=/data/hadoop_repo/logs/hadoop
    ```
- 修改 `etc/core-site.xml`，注意 `fs.defaultFS` 属性中的主机名需要和你配置的主机名保持一致：
    ```xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop001:9000</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/data/hadoop_repo</value>
        </property>
    </configuration>
    ```
- 修改 `etc/hdfs-site.xml`，把 hdfs 中文件副本的数量设置为 2，小于集群的节点数
    ```xml
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>hadoop001:50090</value>
        </property>
    </configuration>
    ```    
- 修改 `etc/mapred-site.xml`，设置 mapreduce 使用的资源调度框架
    ```xml
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>
    ```
- 修改 `etc/yarn-site.xml`，设置 yarn 上支持运行的服务和环境变量白名单，`yarn.resourcemanager.hostname` 配置主节点
    ```xml
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.env-whitelist</name>
            <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CL ASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
        </property>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>hadoop001</value>
        </property>
    </configuration>
    ```
- 修改 `etc/workers` 文件，增加所有从节点的主机名，一个一行
    ```
    hadoop002
    hadoop003
    ```
- 修改脚本：`sbin/start-dfs.sh, sbin/stop-dfs.sh`，增加如下内容：
    ```
    HDFS_DATANODE_USER=root
    HDFS_DATANODE_SECURE_USER=hdfs
    HDFS_NAMENODE_USER=root
    HDFS_SECONDARYNAMENODE_USER=root
    ```
- 修改脚本：`sbin/start-yarn.sh, sbin/stop-yarn.sh`，增加如下内容：
    ```
    YARN_RESOURCEMANAGER_USER=root
    HADOOP_SECURE_DN_USER=yarn
    YARN_NODEMANAGER_USER=root
    ```
- 回到hadoop安装包的上级目录，把 `hadoop001` 节点上修改好配置的安装包拷贝到其他两个从节点：
    ```
    [root@hadoop100 soft]# scp -rq hadoop-3.2.0 hadoop002:/data/soft/ 
    [root@hadoop100 soft]# scp -rq hadoop-3.2.0 hadoop003:/data/soft/
    ```
- 在`hadoop001`节点上格式化 namenode：`bin/hdfs namenode -format`
- 启动集群，在 hadoop001 节点上执行下面命令：`sbin/start-all.sh`，启动和停止都只需要在 hadoop001 节点上操作
    ```
    [root@hadoop001 hadoop-3.2.0]# sbin/start-all.sh
    Starting namenodes on [hadoop001]
    Starting datanodes
    Starting secondary namenodes [hadoop001]
    Starting resourcemanager
    Starting nodemanagers
    ```
- 验证集群：分别在三台机器上执行jps命令：
    ```
    [root@hadoop001 hadoop-3.2.0]# jps
    10627 NameNode
    10900 SecondaryNameNode
    11480 Jps
    11147 ResourceManager

    [root@hadoop002 hadoop-3.2.0]# jps
    2066 DataNode
    2184 NodeManager
    2286 Jps

    [root@hadoop003 hadoop-3.2.0]# jps
    2113 Jps
    1890 DataNode
    2008 NodeManager
    ```

## 1.3、聚合日志

- 开启yarn的日志聚合功能，把散落在 nodemanager 节点上的日志统一收集管理，方便查看日志；
- 需要修改 `yarn-site.xml` 中的 `yarn.log-aggregation-enable` 和 `yarn.log.server.url`，三台机器都需要修改
    ```xml
    <property> 
        <name>yarn.log-aggregation-enable</name>  
        <value>true</value>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://hadoop001:19888/jobhistory/logs/</value>
    </property>
    ```
- 启动 historyserver： `sbin/mr-jobhistory-daemon.sh start historyserver` 或者 `bin/mapred --daemon start historyserver`

## 1.4、Java访问hdfs

### 1.4.1、连接拒绝

在`hdfs-site.xml`配置文件中加入如下配置
```xml
[root@bluefish hadoop-3.2.0]# vi etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
    </property>
</configuration>
```

### 1.4.2、permission问题

```
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/":root:supergroup:drwxr-xr-x
```
在`hdfs-site.xml`配置文件中加入如下配置
```xml
[root@bluefish hadoop-3.2.0]# vi etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>
```

# 2、HBase环境搭建

## 2.1、版本选择

选择合适的HBase版本，主要版本类型：
- 官网版本：http://archive.apache.org/dist/hbase/
- CDH版本：http://archive.cloudera.com/cdh5/cdh/5/

## 2.2、前提条件

- JDK1.7以上
- Hadoop-2.5.0以上
- zookeeper-3.4.5以上

## 2.3、Hadoop2.x分布式安装配置

[Hadoop环境](#1.1Hadoop2.x环境)

## 2.4、配置zookeeper集群

hbase 依赖zookeeper，一般推荐不使用hbase内置的zookeeper

[配置zookeeper集群](../Java/分布式架构/分布式.md#71集群环境)

## 2.5、配置hbase

- 解压文件
- 修改`conf/hbase-env.sh`配置文件
    - 配置JDK目录：JAVA_HOME=${JAVA_HOME}
    - 配置不使用默认的zookeeper： HBASE_MANAGES_ZK=false，如果使用hbase自带的zookeeper的话，这个值可以不用改；
- 配置`conf/hbase-site.xml`文件
    ```xml
    <configuration>
        <property>
            <name>hbase.tmp.dir</name>
            <value>/opt/modules/hbase-0.98.6-cdh5.3.0/data/tmp</value>
        </property>
        <property>
            <name>hbase.rootdir</name>
            <value>hdfs://bigdata1:9000/hbase</value>
        </property>
        <property>
            <name>hbase.cluster.distributed</name>
            <value>true</value>
        </property>
        <!-- HBASE_MANAGES_ZK=true 配置该值，指向具体的zookeeper的数据目录，有可以使用默认值 -->
        <property>
            <name>hbase.zookeeper.property.dataDir</name>
            <value>/data/soft/data/zookeeper</value>
        </property>
        <!-- HBASE_MANAGES_ZK=false时，需要 配置的是zookeeper集群，如果是false的话，该值可以不用配置 -->
        <property>
            <name>hbase.zookeeper.quorum</name>
            <value>zookeeper-node-1,zookeeper-node-2,zookeeper-node-3</value>
        </property>
    </configuration>
    ```
- 配置 `conf/regionservers`文件：一般是使用主机名
- 启动hbase：`bin/start-hbase.sh`

# 3、Hive环境配置

## 3.1、版本选择

选择合适的HBase版本，主要版本类型：
- 官网版本：http://archive.apache.org/dist/hbase/
- CDH版本：http://archive.cloudera.com/cdh5/cdh/5/

## 3.2、前提条件

- CentOS7
- JDK8
- Hadoop-2.6.0-cdh5.7.0
- Hive-1.1.0-cdh5.7.0

## 3.3、Hadoop环境安装

[Hadoop环境](#1.1Hadoop2.x环境)

## 3.4、Hive安装

- 解压缩文件到对应的目录：`tar -zxf hive-1.1.0-cdh5.7.0.tar.gz -C /data/soft/`
- 配置系统环境变量：
    ```bash
    export HIVE_HOME=/data/soft/hive-1.1.0-cdh5.7.0
    export PATH=$PATH:$HIVE_HOME/bin
    ```
- 配置：`conf/hive-env.sh`：
    ```
    HADOOP_HOME=hadoop的安装目录
    ```
- 如果hive依赖mysql的话，需要将安装mysql，[安装方法](Linux环境.md#一CentOS安装mysql数据库)
- 在conf目录下增加配置文件：`hive-site.xml`，配置如下内容：
    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://localhost:3306/sparksql?createDatabaseIfNotExist=true</value>
        </property>
        <!-- mysql驱动 -->
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        </property>
        <!-- mysql数据库连接用户名 -->
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>root</value>
        </property>
        <!-- mysql数据连接密码 -->
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>123456</value>
        </property>
    </configuration>
    ```
    hive没有mysql的驱动，需要拷贝mysql驱动到`$HIVE_HOME/lib/`
- 启动hive：`bin/hive`

## 3.5、集成Tez

- 下载依赖包：[Tez](http://tez.apache.org)，比如这里下载的是0.9.1的版本

- 将`apache-tez-0.9.1-bin.tar.gz`上传到HDFS的/tez目录下
    ```
    hadoop fs -put /opt/software/apache-tez-0.9.1-bin.tar.gz/ /tez
    ```
- 解压缩apache-tez-0.9.1-bin.tar.gz

- 进入到Hive的配置目录：`/opt/module/hive/conf`

- 在Hive的/opt/module/hive/conf下面创建一个`tez-site.xml`文件
    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>tez.lib.uris</name>
            <value>${fs.defaultFS}/tez/apache-tez-0.9.1-bin.tar.gz</value>
        </property>
        <property>
            <name>tez.use.cluster.hadoop-libs</name>
            <value>true</value>
        </property>
        <property>
            <name>tez.history.logging.service.class</name>
            <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
        </property>
    </configuration>
    ```
- 在`hive-env.sh`文件中添加tez环境变量配置和依赖包环境变量配置
    ```sh
    export TEZ_HOME=/root/data-warehouse/tez-0.9.1
    export TEZ_JARS=""
    for jar in `ls $TEZ_HOME |grep jar`; do
        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
    done
    for jar in `ls $TEZ_HOME/lib`; do
        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
    done

    export HIVE_AUX_JARS_PATH=/root/data-warehouse/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS
    ```
- 在hive-site.xml文件中添加如下配置，更改hive计算引擎
    ```xml
    <property>
        <name>hive.execution.engine</name>
        <value>tez</value>
    </property>
    ```

## 3.6、集成Tez的问题

运行Tez时检查到用过多内存而被NodeManager杀死进程问题
```
Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with  exitCode: -103
For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt.
Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container.
```
这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。

**解决办法：关掉虚拟内存检查，修改yarn-site.xml，重启hadoop集群**
```xml
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```
    

# 4、Spark编译安装

- [Spark2.4.7官方编译](http://spark.apache.org/docs/2.4.7/building-spark.html)

## 4.1、前置准备

- CentOS7
- JDK7+
- Maven3.3.9
- Hadoop，比如选择的版本是：Hadoop-2.6.0-cdh5.7.0

## 4.2、编译Spark

- 下载Spark源码包，这里使用的是2.1.0：spark2.1.0
- 解压缩包
- 进入到解压缩后的Spark目录，执行如下命令：
    ```
    ./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0
    ```
    `--name`：表示指定的编译后的文件名称
    `--tgz`：指定编译后的包文件名称
    `-P`：后面跟上指定的hadoop版本等
- 编译执行完成后，会生成如下tgz文件：spark-$VERSION-bin-$NAME.tgz，比如：`spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz`

## 4.3、Spark编译存在问题

如果在编译过程中，你看到的异常信息不是太明显/看不太懂
编译命令后 -X ，就能看到更详细的编译信息

### 4.3.1、仓库问题

报错信息如下：
```
Failed to execute goal on project spark-launcher_2.11: 
Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.1.0: 
Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 
in central (https://repo1.maven.org/maven2) -> [Help 1]
```
解决方案：
```xml
<repository>
    <id>cloudera</id>
    <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
</repository>
```

### 4.3.2、内存不足

信息如下：
```
[info] Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000e8f84000, 18800640, 0) failed; error='无法分配内存' (errno=12)
[info] #
[info] # There is insufficient memory for the Java Runtime Environment to continue.
[info] # Native memory allocation (malloc) failed to allocate 18800640 bytes for committing reserved memory.
[info] # An error report file with more information is saved as:
[info] # /home/hadoop/source/spark-2.1.0/hs_err_pid4764.log
```
解决方案
```
export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m -XX:MaxPermSize=512M"
```

### 4.3.3、scala编译版本问题

如果编译的是scala版本是2.10

`./dev/change-scala-version.sh 2.10`

如果是2.11

`./dev/change-scala-version.sh 2.11`

## 4.4、安装Spark

将上面编译后的包，比如：`spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz`解压缩到对应的目录下

Spark有三种运行模式：
- local：启动命令 `spark-shell --master local[2]`
- standalone


